{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 27 â€“ CSCI 3022\n",
    "\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/fa23/acks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Big font helper\n",
    "def adjust_fontsize(size=None):\n",
    "    SMALL_SIZE = 8\n",
    "    MEDIUM_SIZE = 10\n",
    "    BIGGER_SIZE = 12\n",
    "    if size != None:\n",
    "        SMALL_SIZE = MEDIUM_SIZE = BIGGER_SIZE = size\n",
    "\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## I). Boba Tea:  Two Constant Models, Fit to Different Losses\n",
    "\n",
    "### Exploring MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boba = np.array([20, 21, 22, 29, 33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the $L_1$ loss for a **single** observation. We'll plot the $L_1$ loss for the first observation; since $y_1 = 20$, we'll be plotting\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "L_1(20, \\theta_0) = |20 - \\theta_0|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(10, 30, 1000)\n",
    "l1_loss_single_obvs = np.abs(boba[0] - thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default') # Revert style to default mpl\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(thetas, l1_loss_single_obvs,  'g--', );\n",
    "plt.xlabel(r'$\\theta_0$');\n",
    "plt.ylabel(r'L1 Loss for $y = 20$');\n",
    "# plt.savefig('l1_loss_single_obs.png', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_constant(theta_0, data):\n",
    "    return np.mean(np.array([np.abs(y_obs - theta_0) for y_obs in data]), axis=0)\n",
    "\n",
    "thetas = np.linspace(10, 40, 1000)\n",
    "l1_loss_thetas = mae_constant(thetas, boba)\n",
    "plt.plot(thetas, l1_loss_thetas, color = 'green', lw=3);\n",
    "plt.xlabel(r'$\\theta_0$');\n",
    "plt.ylabel(r'MAE across all data');\n",
    "# plt.savefig('l1_loss_average.png', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Minimizes MAE for the Constant Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we're in a weird style state\n",
    "sns.set_theme()\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "yobs = boba\n",
    "thetahat = np.median(yobs)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 1.5))\n",
    "sns.rugplot(yobs, height=0.25, lw=2) ;\n",
    "plt.scatter([thetahat], [-.1], color='green', lw=4, label=r\"$\\hat{\\theta}_0$\");\n",
    "plt.xlabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.yticks([])\n",
    "# plt.savefig('boba_rug.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_constant(theta, data):\n",
    "    return np.mean(np.array([(y_obs - theta) ** 2 for y_obs in data]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Constant Models, Fit to Different Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default') # Revert style to default mpl\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "nplots = 2\n",
    "def plot_losses(data, title=None, theta_range=(10, 40)):\n",
    "    thetas = np.linspace(theta_range[0], theta_range[1], 1000)\n",
    "    l2_loss_thetas = mse_constant(thetas, data)\n",
    "    thetahat_mse = np.mean(data)\n",
    "\n",
    "    l1_loss_thetas = mae_constant(thetas, data)\n",
    "    thetahat_mae = np.median(data)\n",
    "\n",
    "    fig, axs = plt.subplots(1, nplots, figsize=(5*2+0.5, 3.5))\n",
    "    axs[0].plot(thetas, l2_loss_thetas, lw=3);\n",
    "    axs[0].scatter([thetahat_mse], [mse_constant(thetahat_mse, data)], s=100)\n",
    "    axs[0].annotate(r\"$\\hat{\\theta}_0$ = \" + f\"{thetahat_mse:.1f}\",\n",
    "                    xy=(thetahat_mse, np.average(axs[0].get_ylim())),\n",
    "                    size=20, ha='center', va='top',\n",
    "                    bbox=dict(boxstyle='round', fc='w'))\n",
    "    axs[0].set_xlabel(r'$\\theta_0$');\n",
    "    axs[0].set_ylabel(r'MSE');\n",
    "\n",
    "    axs[1].plot(thetas, l1_loss_thetas, color = 'green', lw=3);\n",
    "    axs[1].scatter([thetahat_mae], [mae_constant(thetahat_mae, data)], color='green', s=100)\n",
    "    axs[1].annotate(r\"$\\hat{\\theta}_0$ = \" + f\"{thetahat_mae:.1f}\",\n",
    "                    xy=(thetahat_mae, np.average(axs[1].get_ylim())),\n",
    "                    size=20, ha='center', va='top',\n",
    "                    bbox=dict(boxstyle='round', fc='w'))\n",
    "    axs[1].set_xlabel(r'$\\theta_0$');\n",
    "    axs[1].set_ylabel(r'MAE');\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_losses(boba)\n",
    "plt.figure(fig)\n",
    "# plt.savefig('loss_compare.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More loss comparison: Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boba_outlier = np.array([20, 21, 22, 29, 33, 1033])\n",
    "fig = plot_losses(boba_outlier, theta_range=[-10, 300])\n",
    "plt.figure(fig)\n",
    "# plt.savefig('loss_outlier.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness under Different Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boba_even = np.array([20, 21, 22, 29, 33, 35])\n",
    "fig = plot_losses(boba_even)\n",
    "plt.figure(fig)\n",
    "#plt.savefig('loss_unique.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "## II).  Dugongs Part 1: Comparing Two Different Models, Both Fit with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dugongs = pd.read_csv(\"data/dugongs.csv\")\n",
    "data_constant = dugongs[\"Age\"]\n",
    "data_linear = dugongs[[\"Length\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Surfaces\n",
    "\n",
    "Computes constant loss surface. As a reminder, the average loss of the constant model is\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\hat{R}(\\theta_0) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\theta_0)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default') # Revert style to default mpl\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "thetas = np.linspace(-20, 42, 1000)\n",
    "l2_loss_thetas = mse_constant(thetas, data_constant)\n",
    "\n",
    "plt.plot(thetas, l2_loss_thetas)\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'MSE')\n",
    "\n",
    "# Optimal point\n",
    "thetahat = np.mean(data_constant)\n",
    "plt.scatter([thetahat], [mse_constant(thetahat, data_constant)], s=50, label = r\"$\\hat{\\theta}_0$\")\n",
    "plt.legend()\n",
    "# plt.savefig('mse_constant_loss.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes 3D loss surface. As a reminder, the average loss for the SLR model is\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\hat{R}(\\theta_0, \\theta_1) = \\frac{1}{n}\\sum_{i=1}^n (y_i - (\\theta_0 + \\theta_1x))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_linear(theta_0, theta_1, data_linear):\n",
    "    data_x, data_y = data_linear.iloc[:,0], data_linear.iloc[:,1]\n",
    "    return np.mean(np.array([(y - (theta_0+theta_1*x)) ** 2 for x, y in zip(data_x, data_y)]), axis=0)\n",
    "\n",
    "theta_0_values = np.linspace(-80, 20, 80)\n",
    "theta_1_values = np.linspace(-10, 30, 80)\n",
    "mse_values = np.array([[mse_linear(x,y,data_linear) for x in theta_0_values] for y in theta_1_values])\n",
    "\n",
    "# Optimal point\n",
    "# Coding the solution to SLR by hand here:\n",
    "data_x, data_y = data_linear.iloc[:, 0], data_linear.iloc[:, 1]\n",
    "theta_1_hat = np.corrcoef(data_x, data_y)[0, 1] * np.std(data_y) / np.std(data_x)\n",
    "theta_0_hat = np.mean(data_y) - theta_1_hat * np.mean(data_x)\n",
    "\n",
    "# Create the 3D plot\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(theta_0_values, theta_1_values)\n",
    "surf = ax.plot_surface(X, Y, mse_values, cmap='viridis', alpha=0.6)  # Use alpha to make it slightly transparent\n",
    "\n",
    "# Scatter point using matplotlib\n",
    "sc = ax.scatter([theta_0_hat], [theta_1_hat], [mse_linear(theta_0_hat, theta_1_hat, data_linear)],\n",
    "                marker='o', color='red', s=100, label='theta hat')\n",
    "\n",
    "# Create a colorbar\n",
    "cbar = fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10)\n",
    "cbar.set_label('Cost Value')\n",
    "\n",
    "ax.set_title('MSE for different $\\\\theta_0, \\\\theta_1$')\n",
    "ax.set_xlabel('$\\\\theta_0$')\n",
    "ax.set_ylabel('$\\\\theta_1$') \n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "# plt.savefig('mse_linear_loss.png', bbox_inches = 'tight');\n",
    "plt.show()\n",
    "\n",
    "print(\"Theta_1_hat = \",theta_1_hat, \"Theta_0_hat = \", theta_0_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Least Squares Constant Model RMSE:\",\n",
    "          np.sqrt(mse_constant(thetahat, data_constant)))\n",
    "print(\"Least Squares Linear Model RMSE:  \",\n",
    "          np.sqrt(mse_linear(theta_0_hat, theta_1_hat, data_linear)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the RMSE (Root Mean Square Error):\n",
    "* Constant model error is HIGHER than linear model error\n",
    "* Linear model is BETTER than constant model (at least for this metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "This plotting code is left for your reference. We'll mainly look at the figures in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yobs = data_linear[\"Age\"]      # The true observations y\n",
    "xs = data_linear[\"Length\"]     # Needed for linear predictions\n",
    "n = len(yobs)                  # Predictions\n",
    "\n",
    "yhats_constant = [thetahat for i in range(n)]    # Not used, but food for thought\n",
    "yhats_linear = [theta_0_hat + theta_1_hat * x for x in xs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we're in a weird style state\n",
    "sns.set_theme()\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8, 1.5))\n",
    "sns.rugplot(yobs, height=0.25, lw=2) ;\n",
    "plt.axvline(thetahat, color='red', lw=4, label=r\"$\\hat{\\theta}_0$\");\n",
    "plt.legend()\n",
    "plt.yticks([])\n",
    "# plt.savefig('dugong_rug.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we're in a weird style state\n",
    "sns.set_theme()\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.scatterplot(x=xs, y=yobs)\n",
    "plt.plot(xs, yhats_linear, color='red', lw=4)\n",
    "# plt.savefig('dugong_line.png', bbox_inches = 'tight');\n",
    "plt.title(\"Regression Line:  Age = \"+str(round(theta_0_hat,2))+\"+\"+str(round(theta_1_hat,2))+\"(Length)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d9f612-dab1-4494-a100-eb200f6c91c9",
   "metadata": {},
   "source": [
    "## `sklearn`\n",
    "\n",
    "`sklearn` is a popular Python library for building and fitting models.  We'll walk through the general workflow for writing code for `sklearn`. While our examples will be focused on linear models, `sklearn` is highly adaptable for use on other (more complex) kinds of models. We'll see examples of this later in the semester. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787a703-ad56-4ec7-ae1f-01b2989d4355",
   "metadata": {},
   "source": [
    "There are three steps to creating and using a model in `sklearn`. \n",
    "\n",
    "**(1) Initialize an instance of the model class**\n",
    "\n",
    "`sklearn` stores \"templates\" of useful models for machine learning. We begin the modeling process by making a \"copy\" of one of these templates for our own use. Model initialization looks like `ModelClass()`, where `ModelClass` is the type of model we wish to create.\n",
    "\n",
    "For now, let's create a linear regression model using `LinearRegression()`. \n",
    "\n",
    "`my_model` is now an instance of the `LinearRegression` class. You can think of it as the \"idea\" of a linear regression model. We haven't trained it yet, so it doesn't know any model parameters and cannot be used to make predictions. In fact, we haven't even told it what data to use for modeling! It simply waits for further instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ebdf4-c362-4e8f-9ace-7c58959b1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "my_model = lm.LinearRegression()\n",
    "\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e28647-a19a-46e2-9704-64a89b4c53e9",
   "metadata": {},
   "source": [
    "**(2) Train the model using `.fit`**\n",
    "\n",
    "Before the model can make predictions, we will need to fit it to our training data. When we fit the model, `sklearn` will use numerical optimization to determine the optimal model parameters. It will then save these model parameters to our model instance for future use. \n",
    "\n",
    "All `sklearn` model classes include a `.fit` method. This function is used to fit the model. It takes in two inputs: the design matrix, `X`, and the target variable, `y`. \n",
    "\n",
    "Let's start by fitting a model with just one feature: the flipper length. We create a design matrix `X` by pulling out the `\"flipper_length_mm\"` column from the DataFrame. Notice that we use **double brackets** to extract this column. Why double brackets instead of just single brackets? The `.fit` method, by default, expects to receive **2-dimensional** data â€“ some kind of data that includes both rows and columns. Writing `penguins[\"flipper_length_mm\"]` would return a 1D `Series`, causing `sklearn` to error. We avoid this by writing `penguins[[\"flipper_length_mm\"]]` to produce a 2D DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bbfd8-08c1-400d-8be7-d1024f7164f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .fit expects a 2D data design matrix, so we use double brackets to extract a DataFrame\n",
    "X = penguins[[\"flipper_length_mm\"]]\n",
    "y = penguins[\"bill_depth_mm\"]\n",
    "\n",
    "my_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510bea9-15ba-49a8-adb0-fcca38c2f648",
   "metadata": {},
   "source": [
    "And in just three lines of code, our model has run gradient descent to determine the optimal model parameters! Our single-feature model takes the form:\n",
    "\n",
    "$$\\text{bill depth} = \\theta_0 + \\theta_1 \\text{flipper length}$$\n",
    "\n",
    "Note that `LinearRegression` will automatically include an intercept term. \n",
    "\n",
    "The fitted model parameters are stored as attributes of the model instance. `my_model.intercept_` will return the value of $\\hat{\\theta}_0$ as a scalar. `my_model.coef_` will return all values $\\hat{\\theta}_1, \n",
    "\\hat{\\theta}_1, ...$ in an array. Because our model only contains one feature, we see just the value of $\\hat{\\theta}_1$ in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63252e1e-02ef-40de-ae52-c373c98e7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intercept term, theta_0\n",
    "my_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bba83-0da2-4db7-9f1d-9664bff520d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All parameters theta_1, ..., theta_p\n",
    "my_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a2c68-4bc5-42e3-b1ca-529323b79014",
   "metadata": {},
   "source": [
    "**(3) Use the fitted model to make predictions**\n",
    "\n",
    "Now that the model has been trained, we can use it to make predictions! To do so, we use the `.predict` method. `.predict` takes in one argument, the design matrix that should be used to generate predictions. To understand how the model performs on the training set, we would pass in the training data. Alternatively, to make predictions on unseen data, we would pass in a new dataset that wasn't used to train the model.\n",
    "\n",
    "Below, we call `.predict` to generate model predictions on the original training data. As before, we use double brackets to ensure that we extract 2-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f8b82-de61-485e-b374-8a8ccfbfa0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = my_model.predict(penguins[[\"flipper_length_mm\"]])\n",
    "\n",
    "print(f\"The RMSE of the model is {np.sqrt(np.mean((y-y_hat)**2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fe864-be58-4a6f-991a-fc9b7b49a570",
   "metadata": {},
   "source": [
    "What if we wanted a model with two features? \n",
    "\n",
    "$$\\text{bill depth} = \\theta_0 + \\theta_1 \\text{flipper length} + \\theta_2 \\text{body mass}$$\n",
    "\n",
    "We repeat this three-step process by intializing a new model object, then calling `.fit` and `.predict` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0db54-8452-43b5-8b32-73c1416660a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: initialize LinearRegression model\n",
    "two_feature_model = lm.LinearRegression()\n",
    "\n",
    "# Step 2: fit the model\n",
    "X_two_features = penguins[[\"flipper_length_mm\", \"body_mass_g\"]]\n",
    "y = penguins[\"bill_depth_mm\"]\n",
    "\n",
    "two_feature_model.fit(X_two_features, y)\n",
    "\n",
    "# Step 3: make predictions\n",
    "y_hat_two_features = two_feature_model.predict(X_two_features)\n",
    "\n",
    "print(f\"The RMSE of the model is {np.sqrt(np.mean((y-y_hat_two_features)**2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II).  Evaluating the Simple Linear Regression Model\n",
    "\n",
    "We will first define some helper functions to calculate the regression line for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def standard_units(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def correlation(x, y):\n",
    "    return np.mean(standard_units(x) * standard_units(y))\n",
    "\n",
    "def slope(x, y):\n",
    "    return correlation(x, y) * np.std(y) / np.std(x)\n",
    "\n",
    "def intercept(x, y):\n",
    "    return np.mean(y) - slope(x, y)*np.mean(x)\n",
    "\n",
    "def fit_least_squares(x, y):\n",
    "    theta_0 = intercept(x, y)\n",
    "    theta_1 = slope(x, y)\n",
    "    return theta_0, theta_1\n",
    "\n",
    "def predict(x, theta_0, theta_1):\n",
    "    return theta_0 + theta_1*x\n",
    "\n",
    "def compute_mse(y, yhat):\n",
    "    return np.mean((y - yhat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.style.use('default') # Revert style to default mpl\n",
    "NO_VIZ, RESID, RESID_SCATTER = range(3)\n",
    "def least_squares_evaluation(x, y, visualize=NO_VIZ):\n",
    "    # statistics\n",
    "    print(f\"x_mean : {np.mean(x):.2f}, y_mean : {np.mean(y):.2f}\")\n",
    "    print(f\"x_stdev: {np.std(x):.2f}, y_stdev: {np.std(y):.2f}\")\n",
    "    print(f\"r = Correlation(x, y): {correlation(x, y):.3f}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    ahat, bhat = fit_least_squares(x, y)\n",
    "    yhat = predict(x, ahat, bhat)\n",
    "    print(f\"\\ theta_0: {ahat:.2f}, \\ theta_1: {bhat:.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(compute_mse(y, yhat)):.3f}\")\n",
    "\n",
    "    # visualization\n",
    "    fig, ax_resid = None, None\n",
    "    if visualize == RESID_SCATTER:\n",
    "        fig, axs = plt.subplots(1,2,figsize=(8, 3))\n",
    "        axs[0].scatter(x, y)\n",
    "        axs[0].plot(x, yhat)\n",
    "        axs[0].set_title(\"LS fit\")\n",
    "        ax_resid = axs[1]\n",
    "    elif visualize == RESID:\n",
    "        fig = plt.figure(figsize=(4, 3))\n",
    "        ax_resid = plt.gca()\n",
    "    \n",
    "    if ax_resid is not None:\n",
    "        ax_resid.scatter(x, y - yhat, color = 'red')\n",
    "        ax_resid.plot([4, 14], [0, 0], color = 'black')\n",
    "        ax_resid.set_title(\"Residuals\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at four different datasets. Without visualizing the data, let's try fitting the simple linear regression model to these four datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in four different datasets: I, II, III, IV\n",
    "x = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n",
    "y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n",
    "y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n",
    "x4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\n",
    "y4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "\n",
    "anscombe = {\n",
    "    'I': pd.DataFrame(list(zip(x, y1)), columns =['x', 'y']),\n",
    "    'II': pd.DataFrame(list(zip(x, y2)), columns =['x', 'y']),\n",
    "    'III': pd.DataFrame(list(zip(x, y3)), columns =['x', 'y']),\n",
    "    'IV': pd.DataFrame(list(zip(x4, y4)), columns =['x', 'y'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(['I', 'II', 'III', 'IV']):\n",
    "    ans = anscombe[dataset]\n",
    "    x, y = ans[\"x\"], ans[\"y\"]\n",
    "    ahat, bhat = fit_least_squares(x, y)\n",
    "    print(f\"Dataset {dataset}: theta_0: {ahat:.2f}, theta_1: {bhat:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all four datasets have exactly the same fit. \n",
    "\n",
    "In fact, all four of them have the same $\\bar x$, $\\bar y$, $\\sigma_x$, $\\sigma_y$, correlation $r$, and RMSE! If we only look at these statistics, we will probably be inclined to say that these datasets are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['I', 'II', 'III', 'IV']:\n",
    "    print(f\">>> Dataset {dataset}:\")\n",
    "    ans = anscombe[dataset]\n",
    "    fig = least_squares_evaluation(ans['x'], ans['y'], visualize = NO_VIZ)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is only when we visualize the datasets that we realize only one of these four sets of data makes sense to model using SLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize = (10, 10))\n",
    "\n",
    "for i, dataset in enumerate(['I', 'II', 'III', 'IV']):\n",
    "    ans = anscombe[dataset]\n",
    "    axs[i//2, i%2].scatter(ans['x'], ans['y'])\n",
    "    axs[i//2, i%2].set_title(f\"Dataset {dataset}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the residuals will also shed light on the differences among these four datasets.\n",
    "\n",
    "In general the residual plot of a good regression shows no pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['I', 'II', 'III', 'IV']:\n",
    "    print(f\">>> Dataset {dataset}:\")\n",
    "    ans = anscombe[dataset]\n",
    "    fig = least_squares_evaluation(ans['x'], ans['y'], visualize = RESID)\n",
    "    plt.show(fig)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The takeaway here is that you should always visualize your datasets before fitting any models to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dugongs Part 2\n",
    "\n",
    "### Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dugongs = pd.read_csv(\"data/dugongs.csv\")\n",
    "dugongs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yobs = dugongs[\"Age\"]      # The true observations y\n",
    "xs = dugongs[\"Length\"]     # Needed for linear predictions\n",
    "\n",
    "theta_1_hat = np.corrcoef(xs, yobs)[0, 1] * np.std(yobs) / np.std(xs)\n",
    "theta_0_hat = np.mean(yobs) - theta_1_hat * np.mean(xs)\n",
    "yhats_linear = theta_0_hat + theta_1_hat * xs\n",
    "\n",
    "print(\"theta_1_hat:\", theta_1_hat)\n",
    "print(\"theta_0_hat:\", theta_0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(xs, yobs)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "The correlation coefficient is pretty high...but there's an issue.\n",
    "\n",
    "Let's first plot the Dugong linear fit again. It doesn't look so bad if we see it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we're in a weird style state\n",
    "sns.set_theme()\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.scatterplot(x=xs, y=yobs)\n",
    "plt.plot(xs, yhats_linear, color='red', lw=4)\n",
    "# plt.savefig('dugong_line.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further inspect by plotting residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = yobs - yhats_linear\n",
    "\n",
    "sns.scatterplot(x=xs, y=residuals, color='red', lw=4)\n",
    "plt.axhline(0, color='black')\n",
    "plt.ylabel(r\"$y - \\hat{y}$\")\n",
    "# plt.savefig('dugong_residuals.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could fit a line to the linear model that relates $ z = \\log(y)$ to $x$:\n",
    "\n",
    "$$ \n",
    "\\Large\n",
    "\\hat{z} = \\theta_0 + \\theta_1 x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dugongs[\"Log(Age)\"] = np.log(dugongs[\"Age\"])\n",
    "zobs = dugongs[\"Log(Age)\"]      # The LOG of true observations y\n",
    "xs = dugongs[\"Length\"]     # Needed for linear predictions\n",
    "\n",
    "ztheta_1_hat = np.corrcoef(xs, zobs)[0, 1] * np.std(zobs) / np.std(xs)\n",
    "ztheta_0_hat = np.mean(zobs) - ztheta_1_hat * np.mean(xs)\n",
    "zhats_linear = ztheta_0_hat + ztheta_1_hat * xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=xs, y=zobs, color='green')\n",
    "plt.plot(xs, zhats_linear, lw=4)\n",
    "# plt.savefig('dugong_zline.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zresiduals = zobs - zhats_linear\n",
    "\n",
    "sns.scatterplot(x=xs, y=zresiduals, color='red', lw=4)\n",
    "plt.axhline(0, color='black')\n",
    "plt.ylabel(r\"$z - \\hat{z}$\")\n",
    "# plt.savefig('dugong_zresiduals.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map back to the original coordinates\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{z} &= \\theta_0 + \\theta_1 x\\\\\n",
    "\\widehat{\\log(y)}&= \\theta_0 + \\theta_1 x\\\\\n",
    "e^{\\widehat{\\log(y)}}&= e^{\\theta_0 + \\theta_1 x}\\\\\n",
    "\\hat{y}&=e^{\\theta_0 + \\theta_1 x}\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.exp(zhats_linear)\n",
    "sns.scatterplot(x=xs, y=yobs)\n",
    "plt.plot(xs, ypred, color='green', lw=4)\n",
    "# plt.savefig('dugong_curve.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
